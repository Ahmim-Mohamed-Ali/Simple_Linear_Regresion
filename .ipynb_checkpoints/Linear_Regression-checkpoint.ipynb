{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e2b93b0-b9e2-4921-b20f-898c558bc2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa36d88a-e471-4546-90e9-3ccf8741e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Charge Notre dataSet\n",
    "x_train = np.array([1.0, 2.0])   #features\n",
    "y_train = np.array([300.0, 500.0])   #target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a20f19c-61b2-412f-8794-98dbf25519b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(x, y, w, b):\n",
    "   \n",
    "    m = x.shape[0] \n",
    "    cost = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = cost + (f_wb - y[i])**2\n",
    "    total_cost = 1 / (2 * m) * cost\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddd6308c-c3be-42f9-9e69-ace73d47f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Calcule Les Gradients pour une regression lineaire\n",
    "    Arguments:\n",
    "      x (ndarray (m,)): Dataset Exemples\n",
    "      y (ndarray (m,)): Target Variables\n",
    "      w,b (scalaire)    : Param√®tres du modeles  \n",
    "    Returns:\n",
    "      dj_dw (scalaire): Le gradient avec le cout pour le parametre w\n",
    "      dj_db (scalaire): Le gradient avec le cout pour le parametre b     \n",
    "     \"\"\"\n",
    "    \n",
    "    # Nombre D'exmple d'entrainements\n",
    "    m = x.shape[0]    \n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):  \n",
    "        f_wb = w * x[i] + b \n",
    "        dj_dw_i = (f_wb - y[i]) * x[i] \n",
    "        dj_db_i = f_wb - y[i] \n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i \n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83195907-c71c-4658-8a78-cf80f155b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n",
    "    \"\"\"\n",
    "    Performs gradient descent to fit w,b. Updates w,b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \"\"\"\n",
    "    \n",
    "    # Un tableau pour stocker le cout J \n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calcule le gradient et met a jour les parametres avec la fonction gradient_function passe en parametre\n",
    "        dj_dw, dj_db = gradient_function(x, y, w , b)     \n",
    "\n",
    "        # Mise A Jours Des Parametres w et b\n",
    "        b = b - alpha * dj_db                            \n",
    "        w = w - alpha * dj_dw                            \n",
    "\n",
    "        # Sauvegarde le cout J a chaque iterations\n",
    "        if i<100000:      \n",
    "            J_history.append( cost_function(x, y, w , b))\n",
    "            p_history.append([w,b])\n",
    "        # Affiche le cout de la fonction avec les parametres b et w de l'iteration i toutes les 10 iterations\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    " \n",
    "    return w, b, J_history, p_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f96ed3-0ba6-406b-9aa9-81c0e07f21b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise les parametres w et b \n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# Parametres de la function gradient_descent\n",
    "iterations = 10000\n",
    "tmp_alpha = 1.0e-2\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n",
    "                                                    iterations, compute_cost, compute_gradient)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
